{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Policy Gradient Tutorial: Solving CartPole with REINFORCE Algorithm  \n",
    "\n",
    "Welcome to this tutorial on **Policy Gradient algorithms**, a fundamental approach in reinforcement learning (RL). This notebook is designed to provide a clear, hands-on guide to implementing and understanding policy gradient methods, specifically applied to the popular **CartPole environment** from OpenAI's Gym. üöÄ  \n",
    "\n",
    "We are going to implement the **REINFORCE algorithm** introduced in the following paper:  \n",
    "üìú Williams, R.J. *Simple statistical gradient-following algorithms for connectionist reinforcement learning*. *Mach Learn* **8**, 229‚Äì256 (1992). [DOI Link](https://doi.org/10.1007/BF00992696)  \n",
    "\n",
    "---\n",
    "\n",
    "## üåü What You'll Learn  \n",
    "\n",
    "In this tutorial, you will:  \n",
    "- üß† **Understand** the core principles behind policy gradient algorithms.  \n",
    "- üíª **Implement** a policy gradient method from scratch using Python.  \n",
    "- üéÆ **Train** an agent to balance a pole on a cart in the CartPole environment.  \n",
    "- üìä **Gain insights** into the advantages and limitations of policy gradient methods in reinforcement learning.  \n",
    "\n",
    "Whether you're a beginner in RL or looking to solidify your understanding of policy-based methods, this notebook aims to bridge the gap between theory and practice.  \n",
    "\n",
    "---\n",
    "\n",
    "## üë®‚Äçüíª About the Author  \n",
    "\n",
    "This tutorial was created by **Salar Basiri**, a PhD student specializing in **Control and Optimization** at the **University of Illinois at Urbana-Champaign**. üìö  \n",
    "\n",
    "Feel free to reach out at:  \n",
    "üìß **salarbsr.1996 [at] gmail** for feedback, questions, or collaborations!  \n",
    "\n",
    "---\n",
    "\n",
    "‚ú® **Let‚Äôs dive in and learn reinforcement learning by solving CartPole!** üéâ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Understanding the REINFORCE Algorithm  \n",
    "\n",
    "The **REINFORCE algorithm** is a fundamental approach in policy gradient methods for reinforcement learning. It provides a way to optimize policies by estimating the gradient of the expected return with respect to the policy parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## üìú Mathematical Derivation  \n",
    "\n",
    "Let **œÑ(1), ..., œÑ(n)** be *n* empirical trajectory samples obtained by running the policy **œÄŒ∏** for *n* episodes, each with **T** time steps. The objective is to maximize the expected return, denoted as **Œ∑(Œ∏)**:  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\eta(\\theta) = \\mathbb{E}_{\\tau \\sim P_\\theta} [f(\\tau)]$,\n",
    "\n",
    "\n",
    "where **f(œÑ)** is a reward function associated with trajectory **œÑ**, and **PŒ∏(œÑ)** represents the probability of the trajectory under the current policy.\n",
    "\n",
    "The gradient of **Œ∑(Œ∏)** with respect to the policy parameters **Œ∏** can be written as:  \n",
    "\n",
    "$\n",
    "\\nabla_\\theta \\eta(\\theta) = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim P_\\theta} [f(\\tau)] = \\mathbb{E}_{\\tau \\sim P_\\theta} [\\nabla_\\theta \\log P_\\theta(\\tau) \\cdot f(\\tau)].\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Gradient Estimation  \n",
    "\n",
    "Using **n** empirical samples from **PŒ∏**, we estimate the gradient as:\n",
    "\n",
    "$\n",
    "\\nabla_\\theta \\eta(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_\\theta \\log P_\\theta(\\tau^{(i)}) \\cdot f(\\tau^{(i)}),\n",
    "$\n",
    "\n",
    "where:  \n",
    "- **œÑ(i)** is the *i-th* trajectory sampled from the policy.  \n",
    "- **log PŒ∏(œÑ(i))** is the log probability of the trajectory under the policy **œÄŒ∏**.  \n",
    "- **f(œÑ(i))** is the return (cumulative reward) for trajectory **œÑ(i)**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Decomposing Log Probability  \n",
    "\n",
    "Here, recall that **Œº(s‚ÇÄ)** denotes the density of the initial state distribution **s‚ÇÄ**. It follows that:\n",
    "\n",
    "$\\log P_\\theta(\\tau) = \\log \\mu(s_0) + \\log \\pi_\\theta(a_0 | s_0) + \\log P_{s_0 a_0}(s_1) + \\log \\pi_\\theta(a_1 | s_1) + \\log P_{s_1 a_1}(s_2) + \\dots + \\log P_{s_{T-1} a_{T-1}}(s_T).$\n",
    "\n",
    "Taking the gradient with respect to Œ∏, we obtain:\n",
    "$\\nabla_\\theta \\log P_\\theta(\\tau) = \\nabla_\\theta \\log \\pi_\\theta(a_0 | s_0) + \\nabla_\\theta \\log \\pi_\\theta(a_1 | s_1) + \\dots + \\nabla_\\theta \\log \\pi_\\theta(a_{T-1} | s_{T-1}).$\n",
    "\n",
    "This decomposition emphasizes that only the terms involving the policy œÄŒ∏ depend on the parameter Œ∏, simplifying the computation.\n",
    "\n",
    "# üîë Final Gradient Expression  \n",
    "\n",
    "Finally, we arrive at the complete expression for the gradient of the expected return **Œ∑(Œ∏)**:  \n",
    "\n",
    "\n",
    "$\\nabla_\\theta \\eta(\\theta) = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim P_\\theta} [f(\\tau)] = \\mathbb{E}_{\\tau \\sim P_\\theta} \\left[ \\left( \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\right) \\cdot f(\\tau) \\right].$\n",
    "\n",
    "And we can estimate this expectation by sampling $N$ trajectories and taking the average.\n",
    "\n",
    "## üöÄ Key Insights  \n",
    "\n",
    "1. **Policy Gradient Theorem**: The REINFORCE algorithm leverages the relationship between the gradient of the expected return and the gradient of the policy's log-likelihood.  \n",
    "2. **Empirical Estimation**: The gradient is estimated using trajectories sampled directly from the policy, enabling model-free reinforcement learning.  \n",
    "3. **Scalability**: While simple and effective, REINFORCE suffers from high variance, often mitigated by techniques such as baseline subtraction.\n",
    "\n",
    "---\n",
    "\n",
    "**Let‚Äôs proceed to implement the REINFORCE algorithm and apply it to the CartPole environment! üéâ**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VVuNNUPkOErF"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "import keyboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cartpole environment"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è CartPole Environment Overview\n",
    "\n",
    "The **CartPole** environment is one of the classic problems used to demonstrate reinforcement learning (RL) algorithms. In this environment, the objective is to balance a pole on a cart by applying forces to the cart, keeping the pole upright for as long as possible. The problem is a typical example of a *control problem* where the agent learns to make decisions to maintain stability in a dynamic system.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## üåü Key Elements of the CartPole Problem\n",
    "\n",
    "- **Cart**: The cart moves horizontally along a straight track. It can be controlled by applying forces to the left or right.\n",
    "- **Pole**: The pole is attached to the cart by a hinge and starts in an upright position. The agent's goal is to keep the pole balanced vertically.\n",
    "- **State Space**: The state space consists of the following four values:\n",
    "  1. **Cart Position**: The position of the cart on the track.\n",
    "  2. **Cart Velocity**: The velocity of the cart.\n",
    "  3. **Pole Angle**: The angle of the pole from the vertical position.\n",
    "  4. **Pole Velocity at Tip**: The velocity of the tip of the pole.\n",
    "\n",
    "- **Action Space**: The action space consists of two possible actions:\n",
    "  1. **Move Left**: Apply a force to move the cart to the left.\n",
    "  2. **Move Right**: Apply a force to move the cart to the right.\n",
    "\n",
    "- **Reward**: The agent receives a reward of +1 for each time step that the pole remains balanced (i.e., stays upright).\n",
    "\n",
    "- **Termination Condition**: The episode terminates when:\n",
    "  1. The pole angle exceeds a certain threshold (usually ¬±15 degrees).\n",
    "  2. The cart moves too far from the center of the track (usually beyond ¬±2.4 units).\n",
    "  3. The maximum number of time steps is reached (typically 500 steps).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Environment Dynamics\n",
    "\n",
    "The environment is designed to simulate a dynamic system where the actions taken by the agent affect the state of the system over time. The agent learns how to balance the pole by adjusting its actions based on the state feedback (cart position, pole angle, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why CartPole?\n",
    "\n",
    "The CartPole environment is widely used in reinforcement learning as a benchmark for evaluating and comparing different algorithms, especially those based on **policy gradients**. It‚Äôs simple yet rich enough to test the effectiveness of RL models in control tasks.\n",
    "\n",
    "Let's dive deeper into solving this problem using the REINFORCE algorithm! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTGisZxqOKJZ",
    "outputId": "e334a22f-e1ba-4303-faf5-2db906572f9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)  # CartPole has continuous states, discrete actions\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only use a simple FC Neural Network to represent our policy given a state. The output of this network is a probability distribution over all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DDbRqf3AOPMc"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write down the code that trains the network by implementing REINFORCE Algorithm introduced above. Here T is the horizon, N is the number of trajectories sampled to estimate the gradient expectation, and gamma is the discount factor. The parameter lr specifies the learning rate of the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy_net, env, writer, baseline = 'None', num_episodes=1000, T=200, N=10, gamma=0.99, lr=0.01):\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes),colour='green'):\n",
    "        trajectories = []\n",
    "\n",
    "        # Step 1: Sample N trajectories\n",
    "        for _ in range(N):\n",
    "            state, _ = env.reset()\n",
    "            trajectory = []\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            t = 0\n",
    "            \n",
    "            while not done and t < T:\n",
    "                # Get action probabilities from the policy network\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action_probs = policy_net(state_tensor)\n",
    "                \n",
    "                # Sample action based on probabilities\n",
    "                action = np.random.choice(len(action_probs), p=action_probs.detach().numpy())\n",
    "                \n",
    "                # Take action in the environment\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                trajectory.append((state, action, reward))\n",
    "                total_reward += reward\n",
    "                \n",
    "                state = next_state\n",
    "                t += 1\n",
    "\n",
    "            # Store trajectory\n",
    "            trajectories.append((trajectory, total_reward))\n",
    "        avg_reward = np.mean([total_reward for _, total_reward in trajectories])\n",
    "        \n",
    "        writer.add_scalar('Mean Rewards', avg_reward, episode)\n",
    "        # Step 2: Calculate the total reward and discounted rewards for each trajectory\n",
    "        rewards_to_go = []\n",
    "        for trajectory, _ in trajectories:\n",
    "            discounted_rewards = []\n",
    "            cumulative_reward = 0\n",
    "            t = 0\n",
    "            # Calculate the discounted reward for each step in the trajectory\n",
    "            for (_, _, reward) in trajectory:\n",
    "                cumulative_reward = reward  +  cumulative_reward\n",
    "                discounted_rewards.insert(0, cumulative_reward)\n",
    "                t += 1\n",
    "            rewards_to_go.append(discounted_rewards)\n",
    "\n",
    "        # Step 3: Compute gradients for policy updates\n",
    "        policy_loss = 0\n",
    "        for i, (trajectory, trajectory_reward) in enumerate(trajectories):\n",
    "            log_grad_sum = 0  # Accumulate log gradients for each trajectory\n",
    "\n",
    "            # For each timestep in the trajectory\n",
    "            for t, (state, action, _) in enumerate(trajectory):\n",
    "                # Convert state to tensor and get action probabilities\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action_probs = policy_net(state_tensor)\n",
    "                \n",
    "                # Calculate log probability of the chosen action\n",
    "                log_prob = torch.log(action_probs[action])\n",
    "                \n",
    "                # choosing the baseline value\n",
    "                alpha = 0.9\n",
    "                base_value = alpha*avg_reward + (1-alpha)*trajectory_reward\n",
    "                # Accumulate gradient of log probability\n",
    "                log_grad_sum += -log_prob * (rewards_to_go[i][0]-base_value)\n",
    "\n",
    "            # Multiply by the discounted total reward of the trajectory\n",
    "            \n",
    "            policy_loss += log_grad_sum \n",
    "\n",
    "        # Step 4: Update the policy network\n",
    "        policy_loss = policy_loss/len(trajectories)\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Optional: Print progress\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below runs the simulation of the cartpole based on a given policy network \"policy_net\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6vOQIOQGUE_D"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_cartpole_simulation(policy_net):\n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    print(\"Press 'q' to stop the simulation.\")\n",
    "\n",
    "    try:\n",
    "        while not done:\n",
    "            # Check if 'q' is pressed to stop the simulation\n",
    "            if keyboard.is_pressed('q'):\n",
    "                print(\"Simulation interrupted by user.\")\n",
    "                break\n",
    "\n",
    "            # Convert state to tensor and get action probabilities\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs = policy_net(state_tensor)\n",
    "\n",
    "            # Choose action with the highest probability\n",
    "            action = np.argmax(action_probs.detach().numpy())\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        # Keep the window open briefly after the simulation ends\n",
    "        time.sleep(2)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Simulation stopped by KeyboardInterrupt.\")\n",
    "    finally:\n",
    "        env.close()\n",
    "        print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here We set up the parameters and run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PQlNMgrRb3S",
    "outputId": "73affe75-affb-4695-f47d-1ca4f3cb4afa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[32m          \u001b[0m| 1/500 [00:00<01:08,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Reward: 22.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|\u001b[32m‚ñà         \u001b[0m| 51/500 [00:06<01:23,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Average Reward: 57.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|\u001b[32m‚ñà‚ñà        \u001b[0m| 101/500 [00:18<01:50,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Average Reward: 72.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|\u001b[32m‚ñà‚ñà‚ñà       \u001b[0m| 151/500 [00:49<05:26,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150, Average Reward: 294.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|\u001b[32m‚ñà‚ñà‚ñà‚ñà      \u001b[0m| 201/500 [01:59<08:02,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Average Reward: 434.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà     \u001b[0m| 251/500 [03:25<07:21,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250, Average Reward: 500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    \u001b[0m| 301/500 [04:54<05:59,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300, Average Reward: 485.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   \u001b[0m| 351/500 [06:26<04:32,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 350, Average Reward: 500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \u001b[0m| 401/500 [08:00<03:04,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400, Average Reward: 500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \u001b[0m| 451/500 [09:33<01:32,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 450, Average Reward: 500.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 500/500 [11:08<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "learning_rate = 0.1\n",
    "gamma = 0.98  # Discount factor\n",
    "N = 10  # Number of trajectories per update\n",
    "T = 200  # Maximum steps per trajectory\n",
    "policy_net = PolicyNetwork(input_dim, output_dim)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "run_name = 'runs/' + env_name +'/ANN/'+datetime.now().strftime((\"%Y_%m_%d %H_%M_%S\"))\n",
    "writer = SummaryWriter(log_dir=run_name)\n",
    "train(policy_net, env, writer, num_episodes=500, T=500, N=10, gamma=0.99, lr=0.001)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can see how well our policy is trained!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZmArkvHTKzh",
    "outputId": "6be953c3-ec4d-4072-b6f2-59386147d60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to stop the simulation.\n",
      "Total Reward: 1090.0\n"
     ]
    }
   ],
   "source": [
    "run_cartpole_simulation(policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the training, we use Tensorboard. you only need to open up a command shell and type:\n",
    "\n",
    "tensorboard --logdir=[where you have stored the log files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
